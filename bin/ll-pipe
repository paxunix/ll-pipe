#!/usr/bin/env zsh


# $Id$

function usage
{
    ${=TOOL_CAT} <<EOF
NAME
    $(progname) - process input in parallel

SYNOPSIS
    $(progname) [options] command [arg ...]

DESCRIPTION
    Reads line input from stdin in chunks and executes a command on each
    chunk.  Stdout and stderr from processing each chunk is output in the
    same order as the input.

    Conceptually:
        A B C D => command+arguments => A' B' C' D'
    where command+arguments are be applied to A, B, C and D in parallel.

    The command and any filenames passed as arguments should not be
    specified with relative paths.

ABOUT THE COMMAND
    The command is executed for each unique input chunk file (hereinafter
    referred to as INF) as follows:
        - INF's contents are piped to command's stdin.
        - INF is substituted for all occurrences of {} on the command's
          command line.  This allows the command to create extra output
          files with a known, per-chunk suffix for custom collection.
        - command's execution time, exit status, memory usage, etc. are
          written in JSON form to INF.meta.
        - INF is removed only if command returns 0.  All INF* files are kept
          if the command fails.
        - the command's exit status is always written into the INF.done file
          once the command has exitted.

ABOUT THE COLLECTORS
    All specified collectors are run in parallel.  The list of input chunk
    filenames is passed on stdin to each.  A collector can read the contents
    of each suffixed filename it is interested in and must remove the file
    once finished.

OPTIONS
    --clean-all
        Remove all files related to the input chunks, regardless of whether
        or not they were processed succesfully.

    --collector command+args
        Any number of --collector options can be given.  They are run in
        parallel after all input chunks have been processed.  The input
        chunk filenames are passed on stdin to each collector.  If
        command+args contains shell-metacharacters or spaces, wrap it in
        single quotes.

    --lines-per-chunk num
        Optional.  The input data will be split into files with a maximum of
        num lines in each file.  Default is 1000.

    --max-attempts num
        Optional.  Will retry the command num times on the same input chunk
        before giving up.  Default is 5.

    --max-jobs num
        Optional.  Will run the command on num chunks in parallel until all
        input is consumed.  Default is 1.

    --meta-outfile-json file
        Optional.  Meta information (e.g. runtime, exit status, etc.) will
        be collected for various parts of the pipeline and saved to file.

    --retry-delay
        Optional.  Number of seconds to sleep in between retry attempts.
        This is a number or a zsh arithmetic expression that can use the
        'numAttempts' variable, which contains the number of times the
        command has failed.  Default is 0.

    --retry-failure-fatal
        Optional.  If the final retry of a command fails, this script is
        terminated and no further input will be processed.  The default
        behaviour is to continue processing even if one chunk fails
        processing.  This will leave input chunk files lying around if they
        were never processed due to early exit.

OPERANDS
    command     The command to execute for each input chunk.  It receives
                the chunk's data on stdin and is expected to write its
                processed output to stdout.

    arg         Arguments passed to the given command.

EXIT STATUS
    0   Every chunk was successfully processed.
    !0  Processing/collection failed for at least one chunk.
EOF
}   # usage


# Main

enableDebug && setopt xtrace

. ${0:h}/../etc/ll-pipe-fns || return 1

local -a collectors
zparseopts -D -E -- \
    "-collector+:=collectors"

# Remove --collector option names
collectors=( ${collectors:#--collector} )

local -A opts
zparseopts -D -A opts -- \
    "-clean-all" \
    "-lines-per-chunk:" \
    "-max-attempts:" \
    "-max-jobs:" \
    "-meta-outfile-json:" \
    "-retry-delay:" \
    "-retry-failure-fatal" \

if [[ ${1} == -* ]]     # Probably an unknown option
then
    error "Unknown option: ${1}"
    usage
    return 1
fi

if [[ -z ${1} ]]
then
    error "No command was given."
    return 1
fi

# Set default parameters if necessary.
: ${opts[--lines-per-chunk]:=1000}
: ${opts[--max-attempts]:=5}
: ${opts[--max-jobs]:=1}
: ${opts[--meta-outfile-json]:=/dev/null}
: ${opts[--retry-delay]:=0}

# meta-outfile-json must be an absolute path because we will be writing to
# it from within the temporary working dir.
opts[--meta-outfile-json]=$(absPath ${opts[--meta-outfile-json]})

local tempDir=$(${=TOOL_MKTEMP} -d ${TMPDIR:-.}/${$}.${0:t}.XXXXXXXXXX)
if (( ${?} ))
then
    error "Failed to create temporary work directory."
    return 1
fi

builtin cd ${tempDir}

# The collector manager runs in the background and is started before any
# input is actually processed.  It scans every second for the next chunk of
# processed data, and once it's available, passes it to the collectors and
# continues until all chunks are finished.  This gets run in a sublist so
# that the multios redirects are effectively also wait'd on by this process.
# XXX:  would be handy to run all collectors in parallel this way; will have
# to eval in order to get all the multios redirects done.
{
    : CollectorManager  # identifier for killing this subprocess on signal

    local curChunk=1
    while true
    do
        if [[ -e ${curChunk}.done ]]
        then
            print -r -- $(( curChunk++ ))

            # Only stop the loop if the next chunk does not exist and all
            # chunks are done.
            [[ -e chunks-done && ! -e ${curChunk}.done ]] && break
        else
            ${=TOOL_SLEEP} 1
        fi
    done > >(
            # Collect stdout
            ${=TOOL_LL_PIPE_COLLECT_BY_SUFFIX} \
                --successful-only \
                --suffix .out \
                --output-file /dev/stdout
        ) > >(
            # Collect stderr
            ${=TOOL_LL_PIPE_COLLECT_BY_SUFFIX} \
                --successful-only \
                --suffix .err \
                --output-file /dev/stderr
        ) > >(
            # Collect meta files
            ${=TOOL_LL_PIPE_COLLECT_BY_SUFFIX} \
                --suffix .meta \
                --output-file ${opts[--meta-outfile-json]}
        )
} &

# Ensure this background sublist is killed on likely signals.
trap 'kill %?CollectorManager 2>/dev/null' \
    INT HUP TERM USR1 USR2 QUIT ABRT

# Split input into chunks and save the chunk names to a file so we can
# combine everything later.
message -u2 "Splitting input into chunks..."

${=TOOL_LL_PIPE_SPLIT} \
    --lines-per-chunk ${opts[--lines-per-chunk]} \
    > chunk-file-list |    # note the pipe

# Process each chunk.
${=TOOL_XARGS} \
    --max-lines=1 \
    --max-procs=${opts[--max-jobs]} \
    --no-run-if-empty \
    --replace='{}' \
    ${=TOOL_LL_PIPE_WORKER} \
        --max-attempts ${opts[--max-attempts]} \
        --retry-delay ${opts[--retry-delay]} \
        ${(k)opts[--retry-failure-fatal]} \
        --input-file '{}' \
        ${@}

local xargsError=${?}

# Signal the collector manager that all input chunks have been processed.
: > chunks-done

# Run all user-specified collectors.  XXX:  it would be nice to run this
# with the stdout and stderr collectors as well.
local collectorFile=1
for collector in ${collectors}
do
    message -u2 "Running collector ${collector%%[[:space:]]}..."

    timeCommand \
        --kv "action\\tcollector" \
        --output $((collectorFile++)).meta-collect \
        ${(z)collector} < chunk-file-list &

    # XXX: need to capture exit status from each collector and OR it with
    # the xargs status to ultimately only return 0 if nothing in this whole
    # pipline failed.
done

wait        # Wait for all collectors and the collector manager to complete.

${=TOOL_RM} -f chunks-done

# Collect meta files from collectors.  Order doesn't matter.
print -r -l -- *.meta-collector(N) |
    ${=TOOL_LL_PIPE_COLLECT_BY_SUFFIX} \
        --suffix '' \
        --output-file ${OLDPWD}/${opts[--meta-outfile-json]}

# If --clean-all was not given, only clean up list files and .done files
# from successful chunks; otherwise remove the directory and its contents.
if (( ! ${+opts[--clean-all]} ))
then
    print -l -r -- chunk-file-list [0-9]*.done(N) |
    ${=TOOL_XARGS} \
        --max-procs=${opts[--max-jobs]} \
        --no-run-if-empty \
        ${=TOOL_RM} -f

    # Try to remove our temporary dir.  Ignore failure since it may have
    # files from failed chunks in it.
    builtin cd -
    ${=TOOL_RMDIR} --ignore-fail-on-non-empty ${tempDir}
else
    builtin cd -
    ${=TOOL_RM} -fr ${tempDir}
fi

return ${xargsError}

# vim: ft=zsh
